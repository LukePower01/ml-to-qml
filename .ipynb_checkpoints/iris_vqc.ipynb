{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6da9e08b-5d4e-49b7-9ce4-c331b78a1ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c239ad64-203c-48be-9816-78fdab73ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_ibm_provider import IBMProvider\n",
    "provider = IBMProvider()\n",
    "#IBMQ.save_account(\"4a2b1a36805ecf250f886c317cafd3bbdf55633282b8df8b8b08857d20356615132b9d174cc3a31bd54ccabf228f08658ebedf07f8b52c67619bdf0dff703bc7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ecc77e-8cb6-48e1-817d-c58ca2b93497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Qiskit \n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes, EfficientSU2\n",
    "from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier, VQC\n",
    "from qiskit_machine_learning.algorithms.regressors import NeuralNetworkRegressor, VQR\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN, EstimatorQNN\n",
    "from qiskit_machine_learning.circuit.library import QNNCircuit\n",
    "from qiskit.primitives import Sampler\n",
    "from qiskit_algorithms.optimizers import COBYLA, L_BFGS_B\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "\n",
    "# Visualization libraries\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from qiskit.visualization import plot_histogram\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Machine learning and preprocessing\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209adaa0-7a9e-482f-99ba-e0c7b982ed0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc8c15b5-e9f2-4e73-a4a2-ef8f346e7468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95db24af-d686-4eec-aa3d-fbce552bbc2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da222c-3532-443c-b841-e442af7de4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56aa9226-8f99-42b7-9942-f4490f26db3b",
   "metadata": {},
   "source": [
    "Normalize the features; transform all the features onto 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0c1565a-f2cd-4ba2-8df1-7a3bcb66610b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m features \u001b[38;5;241m=\u001b[39m MinMaxScaler()\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mfeatures\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "features = MinMaxScaler().fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc5ef4-3add-46b2-87f7-d3aa2d4d36c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(features, columns=iris_data.feature_names)\n",
    "df['class'] = pd.DataFrame(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303e9ce1-fe21-4167-9e02-99ee0d357716",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_globals.random_seed = 4701\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "features, labels, train_size = 0.8, random_state = algorithm_globals.random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f3af28-b9cf-45ca-8a74-e749366c6299",
   "metadata": {},
   "source": [
    "# Training a QML Model\n",
    "\n",
    "We are training a 'Variational Quanum Classifier', or VQC. It takes a map and an ansatz and constructs a quantum neural network automatically. In the simplest case it is enough to pass the number of qubits and a quantum instance to construct a valid classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5be550-6fca-436a-b027-41147de6ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = features.shape[1]\n",
    "\n",
    "feature_map = ZZFeatureMap(feature_dimension=num_features, reps = 1)\n",
    "feature_map.decompose().draw('mpl', fold=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23c8d5b-86fb-4b48-ac90-669b23e3db54",
   "metadata": {},
   "source": [
    "## ZZFeatureMap\n",
    "The ZZFeatureMap specifically involves the application of ZâŠ—Z interactions, where \"Z\" represents the Pauli-Z operator, applied to pairs of qubits. These interactions, combined with single-qubit rotations that encode the input data, create entangled quantum states that reflect the structure of the input data. The name ZZFeatureMap comes from the use of these ZZ interactions, which are key to the map's ability to capture and exploit correlations in the input data in a way that is uniquely quantum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab329c5-b096-458a-a25e-138cdebddd00",
   "metadata": {},
   "source": [
    "x\\[0\\] ,,, x\\[3\\] are placehoders for the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22209193-f6aa-4942-bdaf-0d36ee93c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "ansatz = RealAmplitudes(num_qubits=num_features, reps = 3)\n",
    "ansatz.decompose().draw('mpl', fold = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35692721-1b6c-400b-b0b5-6a3a24cacac0",
   "metadata": {},
   "source": [
    "The parameters x\\[0\\] to x\\[15\\] are the trainable weights of the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaf7ff9-12ea-46b0-9db2-c0dc3391fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = COBYLA(maxiter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac1915c-1826-4c08-babd-a9c9bb2c9561",
   "metadata": {},
   "source": [
    "Using a simulator to train: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da061aeb-d943-451c-b8a1-2131868023ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Sampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40410b07-6e9f-4992-913f-09533a8d67a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_func_vals = []\n",
    "\n",
    "# objective function characterizes the distance between the predictions and known labeled data.\n",
    "\n",
    "def callback_graph(weights, obj_func_eval):\n",
    "             \n",
    "# Append the value of the objective function to an array so we \n",
    "# can plot the iteration verses the objective function value\n",
    "             \n",
    "             clear_output(wait=True)\n",
    "             objective_func_vals.append(obj_func_eval)\n",
    "             plt.title('Objective function value against Iteration')\n",
    "             plt.xlabel('Iteration')\n",
    "             plt.ylabel('Objective function value')\n",
    "             plt.plot(range(len(objective_func_vals)), objective_func_vals)\n",
    "             \n",
    "             plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c419ba7c-cfc6-45c9-8561-cb08c8b980a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing DataFrames each missing one feature\n",
    "dfs = {\n",
    "    \"Full Features\": df,\n",
    "    \"Missing Sepal Length\": df.drop(columns=['sepal length (cm)']),\n",
    "    \"Missing Sepal Width\": df.drop(columns=['sepal width (cm)']),\n",
    "    \"Missing Petal Length\": df.drop(columns=['petal length (cm)']),\n",
    "    \"Missing Petal Width\": df.drop(columns=['petal width (cm)']),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03db3d32-055b-43ba-bed6-88bb11683d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aadfa56-a2b4-4a5b-b131-bd113ae1a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.iloc[:,:-1]\n",
    "labels = df.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13d5b9d-57f2-4ccb-ab1a-6726de477370",
   "metadata": {},
   "outputs": [],
   "source": [
    "for description, data in dfs.items():\n",
    "    features = data.iloc[:,:-1].values\n",
    "    labels = data.iloc[:,-1:].values\n",
    "    num_features = features.shape[1]  # Adjusted to current feature count\n",
    "    features = MinMaxScaler().fit_transform(features)\n",
    "    print(features[5])\n",
    "    print(description, data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d9277e-59d7-4c2e-9898-f5da2a67f344",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times = {}\n",
    "\n",
    "vqc_scores = {}\n",
    "\n",
    "objective_func_vals_dict = {\n",
    "    \"Full Features\": [],\n",
    "    \"Missing Sepal Length\": [],\n",
    "    \"Missing Sepal Width\": [],\n",
    "    \"Missing Petal Length\": [],\n",
    "    \"Missing Petal Width\": []\n",
    "}\n",
    "\n",
    "for description, data in dfs.items():\n",
    "    features = data.iloc[:,:-1].values\n",
    "    labels = data.iloc[:,-1:].values\n",
    "    num_features = features.shape[1]  # Adjusted to current feature count\n",
    "    features = MinMaxScaler().fit_transform(features)\n",
    "    \n",
    "    # Dynamically adjust the feature map and ansatz for the current number of features\n",
    "    feature_map = ZZFeatureMap(feature_dimension=num_features, reps = 1)\n",
    "    ansatz = RealAmplitudes(num_qubits=num_features, reps = 3)\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    features, labels, train_size = 0.8, random_state = algorithm_globals.random_seed)\n",
    "    optimizer = COBYLA(maxiter=100)\n",
    "    \n",
    "    # Prepare features and labels\n",
    "    current_dataset = description\n",
    "    \n",
    "    # Initialize VQC for the adjusted feature map and ansatz\n",
    "    vqc = VQC(\n",
    "        sampler = sampler, \n",
    "        feature_map = feature_map,\n",
    "        ansatz = ansatz, \n",
    "        optimizer = optimizer, \n",
    "        callback = callback_graph,)\n",
    "    \n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "    vqc.fit(train_features, train_labels)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Scores\n",
    "    train_score_quantum = vqc.score(train_features, train_labels)\n",
    "    test_score_quantum = vqc.score(test_features, test_labels)\n",
    "    vqc_scores[description] = {'Test Score': test_score}\n",
    "\n",
    "    # Store elapsed time for training\n",
    "    training_times[description] = elapsed_time\n",
    "    print(f\"Training time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028cef0a-11ab-4128-be60-bd656cf8e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqc = VQC(\n",
    "    sampler = sampler, \n",
    "    feature_map = feature_map,\n",
    "    ansatz = ansatz, \n",
    "    optimizer = optimizer, \n",
    "    callback = callback_graph,)\n",
    "\n",
    "# Clear objective value history\n",
    "objective_func_vals = []\n",
    "\n",
    "start = time.time()\n",
    "vqc.fit(train_features,train_labels)\n",
    "elsapsed_time = time.time() - start \n",
    "\n",
    "print(f'Training time: {elsapsed_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918689d-c07f-4dd3-925e-c56075cdd807",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score_q4 = vqc.score(train_features, train_labels)\n",
    "test_score_q4 = vqc.score(test_features, test_labels)\n",
    "\n",
    "print(f\"Quantum VQC on the training dataset: {train_score_q4:.2f}\")\n",
    "print(f\"Quantum VQC on the test dataset:     {test_score_q4:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2859ab5-2f19-4058-ae2e-a63f67f0b35f",
   "metadata": {},
   "source": [
    "# Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f038be4a-311f-4a32-9f1d-14ea1177dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = PCA(n_components=2).fit_transform(features)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 6)\n",
    "sns.scatterplot(x=features[:, 0], y=features[:, 1], hue=labels, palette=\"tab10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d2d4af-aed9-40ea-89cf-26d122441edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    features, labels, train_size=0.8, random_state=algorithm_globals.random_seed\n",
    ")\n",
    "\n",
    "svc.fit(train_features, train_labels)\n",
    "\n",
    "train_score_c2 = svc.score(train_features, train_labels)\n",
    "test_score_c2 = svc.score(test_features, test_labels)\n",
    "\n",
    "print(f\"Classical SVC on the training dataset: {train_score_c2:.2f}\")\n",
    "print(f\"Classical SVC on the test dataset:     {test_score_c2:.2f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aca8d7a-d38f-45ce-8f59-01b57e2b845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = features.shape[1]\n",
    "\n",
    "feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
    "ansatz = RealAmplitudes(num_qubits=num_features, reps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8dc124-9fd2-45e8-982a-dc4594592d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = COBYLA(maxiter=40) # reduction of iterations due to fewer qubits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea3a971-224e-4b6b-89ab-dd6c9c9f689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqc = VQC(\n",
    "    sampler=sampler,\n",
    "    feature_map=feature_map,\n",
    "    ansatz=ansatz,\n",
    "    optimizer=optimizer,\n",
    "    callback=callback_graph,\n",
    ")\n",
    "\n",
    "# clear objective value history\n",
    "objective_func_vals = []\n",
    "\n",
    "# make the objective function plot look nicer.\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "vqc.fit(train_features, train_labels)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Training time: {round(elapsed)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39afbfb0-8c58-43cb-a53c-42fc8fdf7875",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score_q2_ra = vqc.score(train_features, train_labels)\n",
    "test_score_q2_ra = vqc.score(test_features, test_labels)\n",
    "\n",
    "print(f\"Quantum VQC on the training dataset using RealAmplitudes: {train_score_q2_ra:.2f}\")\n",
    "print(f\"Quantum VQC on the test dataset using RealAmplitudes:     {test_score_q2_ra:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f33f10-4c72-45b2-b83d-a3a9096253b5",
   "metadata": {},
   "source": [
    "Note the objective function is almost flattening, meaning increasing the number of iterations won't be able to increase the score. We will need to try another ansatz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db83d423-cb7c-4d88-8963-7ff3d95976c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ansatz = EfficientSU2(num_qubits=num_features, reps=3)\n",
    "optimizer = COBYLA(maxiter=40)\n",
    "\n",
    "vqc = VQC(\n",
    "    sampler=sampler,\n",
    "    feature_map=feature_map,\n",
    "    ansatz=ansatz,\n",
    "    optimizer=optimizer,\n",
    "    callback=callback_graph,\n",
    ")\n",
    "\n",
    "# clear objective value history\n",
    "objective_func_vals = []\n",
    "\n",
    "start = time.time()\n",
    "vqc.fit(train_features, train_labels)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Training time: {round(elapsed)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da68cbed-6c2a-4d47-96d5-7f3e77df2748",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score_q2_eff = vqc.score(train_features, train_labels)\n",
    "test_score_q2_eff = vqc.score(test_features, test_labels)\n",
    "\n",
    "print(f\"Quantum VQC on the training dataset using EfficientSU2: {train_score_q2_eff:.2f}\")\n",
    "print(f\"Quantum VQC on the test dataset using EfficientSU2:     {test_score_q2_eff:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd4723-55f9-4c31-a7b9-fa3e1eeb6eda",
   "metadata": {},
   "source": [
    "Better than previous. lets try increase the number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609bf12a-f340-4c26-be6e-7cfd18fc6205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.circuit.library import EfficientSU2\n",
    "\n",
    "ansatz = EfficientSU2(num_qubits=num_features, reps=3)\n",
    "optimizer = COBYLA(maxiter=80)\n",
    "\n",
    "vqc2 = VQC(\n",
    "    sampler=sampler,\n",
    "    feature_map=feature_map,\n",
    "    ansatz=ansatz,\n",
    "    optimizer=optimizer,\n",
    "    callback=callback_graph,\n",
    ")\n",
    "\n",
    "# clear objective value history\n",
    "objective_func_vals = []\n",
    "\n",
    "start = time.time()\n",
    "vqc2.fit(train_features, train_labels)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Training time: {round(elapsed)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcebbc5a-5a7d-4f03-84c9-6281c6d217b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score_q3_eff = vqc2.score(train_features, train_labels)\n",
    "test_score_q3_eff = vqc2.score(test_features, test_labels)\n",
    "\n",
    "print(f\"Quantum VQC on the training dataset using EfficientSU2: {train_score_q3_eff:.2f}\")\n",
    "print(f\"Quantum VQC on the test dataset using EfficientSU2:     {test_score_q3_eff:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac605619-68ce-4989-9bc9-10f3988f5c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model                           | Test Score | Train Score\")\n",
    "print(f\"SVC, 4 features                 | {train_score_c4:10.2f} | {test_score_c4:10.2f}\")\n",
    "print(f\"VQC, 4 features, RealAmplitudes | {train_score_q4:10.2f} | {test_score_q4:10.2f}\")\n",
    "print(f\"----------------------------------------------------------\")\n",
    "print(f\"SVC, 2 features                 | {train_score_c2:10.2f} | {test_score_c2:10.2f}\")\n",
    "print(f\"VQC, 2 features, RealAmplitudes | {train_score_q2_ra:10.2f} | {test_score_q2_ra:10.2f}\")\n",
    "print(f\"VQC, 2 features, EfficientSU2   | {train_score_q2_eff:10.2f} | {test_score_q2_eff:10.2f}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50a0c8b-51bc-4d0f-982b-2866511f2275",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5148a86a-9083-469e-a835-ae55153e4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_globals.random_seed = 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072aec2-6218-4ca3-b8fb-f70ba5510d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['class'] = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb3a12f-08c2-4b4b-9280-88f4926f600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the numeric class labels to the actual species names for clarity\n",
    "class_mapping = dict(zip(range(3), iris.target_names))\n",
    "df['class'] = df['class'].map(class_mapping)\n",
    "\n",
    "# Define the feature names (all features are used in this case)\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Adjusting the number of rows in the subplots to fit the Iris dataset features\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))\n",
    "axes = axes.flatten()  # Flatten to ensure it's iterable\n",
    "\n",
    "# Splitting the data by class\n",
    "dataSetosa = df[df['class'] == 'setosa']\n",
    "dataVersicolor = df[df['class'] == 'versicolor']\n",
    "dataVirginica = df[df['class'] == 'virginica']\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    feature = feature_names[idx]\n",
    "    binwidth = (df[feature].max() - df[feature].min()) / 50  # Adjust binwidth if necessary\n",
    "    # Create histograms\n",
    "    ax.hist([dataSetosa[feature], dataVersicolor[feature], dataVirginica[feature]], \n",
    "            bins=np.arange(df[feature].min(), df[feature].max() + binwidth, binwidth), \n",
    "            alpha=0.5, stacked=True, density=True, label=['Setosa', 'Versicolor', 'Virginica'], color=['r', 'g', 'b'])\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_title(feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('iris_features_hist.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e86b0e-4840-4854-97e7-18026b6c4fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris_data = load_iris()\n",
    "\n",
    "# The Iris dataset is already in a good shape for ML, so we use all features\n",
    "X = iris_data.data\n",
    "y = iris_data.target\n",
    "feature_names = iris_data.feature_names\n",
    "\n",
    "# Initialize the StandardScaler and scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Feature importance from the Random Forest model\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "print(\"Feature importances from Random Forest Classifier:\")\n",
    "for name, importance in zip(feature_names, feature_importances):\n",
    "    print(f\"{name}: {importance:.4f}\")\n",
    "\n",
    "# Evaluate the classifier\n",
    "rf_accuracy = rf_classifier.score(X_test, y_test)\n",
    "print(f\"\\nRandom Forest Test Accuracy: {rf_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683b9c3-e42d-4c91-8ca6-7037533c525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = iris_data.feature_names\n",
    "\n",
    "# Sort the features by importance\n",
    "sorted_idx = np.argsort(feature_importances)[::-1]\n",
    "sorted_feature_names = [feature_names[i] for i in sorted_idx]\n",
    "sorted_importances = [feature_importances[i] for i in sorted_idx]\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_importances)), sorted_importances, align='center')\n",
    "plt.yticks(range(len(sorted_importances)), sorted_feature_names)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importances from Random Forest Classifier on Iris Dataset')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab3f0ad-dd54-4ff2-a55a-b1ea299cb3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 4\n",
    "\n",
    "feature_map = ZZFeatureMap(feature_dimension=num_features, reps = 1)\n",
    "feature_map.decompose().draw('mpl', fold=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe097b83-40e2-4294-86e6-00ffaf5a645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc = QNNCircuit(feature_map=feature_map)\n",
    "estimator_qnn = EstimatorQNN(circuit = qc)\n",
    "estimator_qnn.forward(X[0, :], algorithm_globals.random.random(estimator_qnn.num_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35cf7b3-1536-4124-9df5-5a14820a8cea",
   "metadata": {},
   "source": [
    "using the data with all features as a baseline we can infer the importance of the features by looking the at the difference from the baseline. The larger the decrease the model performance the greater the imporance of the feature for correct classification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe8d57-0f8a-43ec-8ee8-8ac41a852ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "labels = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542213ed-04fe-41ad-bdb9-87b3cb573731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing DataFrames each missing one feature\n",
    "dfs = {\n",
    "    \"Full Features\": df,\n",
    "    \"Missing Sepal Length\": df.drop(columns=['sepal length (cm)']),\n",
    "    \"Missing Sepal Width\": df.drop(columns=['sepal width (cm)']),\n",
    "    \"Missing Petal Length\": df.drop(columns=['petal length (cm)']),\n",
    "    \"Missing Petal Width\": df.drop(columns=['petal width (cm)']),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf46c87b-af53-4cdb-b36c-f249f88abe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Initialize variables for storing scores\n",
    "classical_scores = {}\n",
    "\n",
    "# Setting the global random seed for reproducibility\n",
    "algorithm_globals.random_seed = 4701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b04094-469e-44ff-bdf2-c4dc1dc1c7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafc2837-d109-4b14-912d-76dad9756cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = {}\n",
    "\n",
    "for description, features in dfs.items():\n",
    "    # Splitting the dataset\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "        features, y, train_size=0.8, random_state=algorithm_globals.random_seed)\n",
    "    \n",
    "    # Scaling the features\n",
    "    train_features_scaled = scaler.fit_transform(train_features)\n",
    "    test_features_scaled = scaler.transform(test_features)\n",
    "    \n",
    "    # Initialize variables to store progress for the current dataset variant\n",
    "    iterations = range(1, 36)  # Simulating for 100 iterations\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # Loop over the range of iterations, incrementally training the SVC\n",
    "    for i in iterations:\n",
    "        svc = SVC(max_iter=i)\n",
    "        svc.fit(train_features_scaled, train_labels)  # fit the model\n",
    "        \n",
    "        # Evaluate the model\n",
    "        train_score_classical = svc.score(train_features_scaled, train_labels)\n",
    "        test_score_classical = svc.score(test_features_scaled, test_labels)\n",
    "        \n",
    "        train_scores.append(train_score)\n",
    "        test_scores.append(test_score)\n",
    "\n",
    "    progress[description] = test_scores\n",
    "    \n",
    "    # Store the final scores for comparison\n",
    "    classical_scores[description] = {\n",
    "        \"Training\": train_scores[-1],\n",
    "        \"Test\": test_scores[-1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2643708e-91c6-4f43-90d6-36491d58b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for description, scores in progress.items():\n",
    "    plt.plot(iterations, scores, label=f'{description} Test Accuracy')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('SVC Training Progress Across Dataset Variants')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd9b43-1a54-4a53-9d47-9ea36f39536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_feature_test_score_classical = classical_scores[\"Full Features\"][\"Test\"]\n",
    "\n",
    "classical_differences = {desc: full_feature_test_score_classical - classical_scores[desc][\"Test\"] for desc in classical_scores if desc != \"Full Features\"}\n",
    "descriptions = list(classical_differences.keys())\n",
    "classical_diff_values = list(classical_differences.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b9542-5518-4b4e-9534-3f1830939502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(descriptions, classical_diff_values, color='skyblue')\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Difference in Test Accuracy from Full Features')\n",
    "plt.title('Impact of Missing Features on SVC Test Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "plt.show()\n",
    "print(\"Scores and Differences from Full Features:\")\n",
    "for description, score_dict in classical_scores.items():\n",
    "    test_score_classical = score_dict[\"Test\"]\n",
    "    difference_classical = full_feature_test_score_classical - test_score_classical if description != \"Full Features\" else 0\n",
    "    print(f\"{description}:\")\n",
    "    print(f\"  - Classical SVC on the training dataset: {score_dict['Training']:.2f}\")\n",
    "    print(f\"  - Classical SVC on the test dataset:     {test_score:.2f}\")\n",
    "    print(f\"  - Difference from Full Features test score: {difference:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f941e7-3708-4332-94d3-903002eda5a0",
   "metadata": {},
   "source": [
    "# Quantum VC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1f94d-48e4-4de7-88e8-5ad62b6e1232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dataset = \"Full Features\"  # This should be updated dynamically in your training loop\n",
    "\n",
    "def callback_graph(weights, obj_func_eval):\n",
    "    global current_dataset\n",
    "    clear_output(wait=True)\n",
    "    objective_func_vals_dict[current_dataset].append(obj_func_eval)\n",
    "    plt.title(\"Objective function value against Iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Objective function value\")\n",
    "    \n",
    "    for description, obj_vals in objective_func_vals_dict.items():\n",
    "        if obj_vals:  # Plot only if there are values\n",
    "            iterations = range(len(obj_vals))\n",
    "            plt.plot(iterations, obj_vals, label=description)\n",
    "            # Annotate the most recent point\n",
    "            plt.annotate(f\"{obj_vals[-1]:.2f}\",\n",
    "                         (iterations[-1], obj_vals[-1]),\n",
    "                         textcoords=\"offset points\",\n",
    "                         xytext=(0,10),\n",
    "                         ha='center',\n",
    "                         fontsize=8)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef73254-463d-4cb0-94c8-8e8b7208908b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c641cdb-477e-43db-b7c1-44bb73867602",
   "metadata": {},
   "outputs": [],
   "source": [
    "ansatz = EfficientSU2(num_qubits=num_features, reps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3167af-8167-4bfd-b93f-bbade62a4201",
   "metadata": {},
   "outputs": [],
   "source": [
    "ansatz = RealAmplitudes(num_qubits=num_features, reps = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bb14b6-9e09-4a0a-8fa3-3f0edfee4c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = COBYLA(maxiter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2638d9b-9db6-4843-92d1-d76b671d2a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20941f14-c94a-443d-9e60-0686b6e1d87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times = {}\n",
    "vqc_scores = {}\n",
    "objective_func_vals_dict = {\n",
    "    \"Full Features\": [],\n",
    "    \"Missing Sepal Length\": [],\n",
    "    \"Missing Sepal Width\": [],\n",
    "    \"Missing Petal Length\": [],\n",
    "    \"Missing Petal Width\": []\n",
    "}\n",
    "for description, data in dfs.items():\n",
    "    features = data.iloc[:,:-1].values\n",
    "    labels = data.iloc[:,-1:].values\n",
    "    num_features = features.shape[1]  # Adjusted to current feature count\n",
    "    features = MinMaxScaler().fit_transform(features)  # Adjusted to current feature count\n",
    "    \n",
    "    # Dynamically adjust the feature map and ansatz for the current number of features\n",
    "    feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
    "    ansatz = EfficientSU2(num_qubits=num_features, reps=3)\n",
    "    \n",
    "    # Prepare features and labels\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "        features_scaled, labels, train_size=0.8, random_state=algorithm_globals.random_seed)\n",
    "    \n",
    "    current_dataset = description\n",
    "    \n",
    "    # Initialize VQC for the adjusted feature map and ansatz\n",
    "    vqc = VQC(\n",
    "        feature_map=feature_map,\n",
    "        ansatz=ansatz,\n",
    "        optimizer=optimizer,\n",
    "        callback=callback_graph\n",
    "    )\n",
    "    \n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "    vqc.fit(train_features, train_labels)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Scores\n",
    "    train_score_quantum = vqc.score(train_features, train_labels)\n",
    "    test_score_quantum = vqc.score(test_features, test_labels)\n",
    "    vqc_scores[description] = {'Training Score': train_score_quanum,'Test Score': test_score_quantum}\n",
    "\n",
    "    # Store elapsed time for training\n",
    "    training_times[description] = elapsed_time\n",
    "    print(f\"Training time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e5ff51-e127-458c-a482-970428c9dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times = {}\n",
    "vqc_scores = {}\n",
    "objective_func_vals_dict = {\n",
    "    \"Full Features\": [],\n",
    "    \"Missing Sepal Length\": [],\n",
    "    \"Missing Sepal Width\": [],\n",
    "    \"Missing Petal Length\": [],\n",
    "    \"Missing Petal Width\": []\n",
    "}\n",
    "for description, data in dfs.items():\n",
    "    features = data.iloc[:,:-1].values\n",
    "    labels = data.iloc[:,-1:].values\n",
    "    num_features = features.shape[1]  # Adjusted to current feature count\n",
    "    features = MinMaxScaler().fit_transform(features)  # Adjusted to current feature count\n",
    "    \n",
    "    # Dynamically adjust the feature map and ansatz for the current number of features\n",
    "    feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
    "    ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
    "    \n",
    "    # Prepare features and labels\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "        features_scaled, labels, train_size=0.8, random_state=algorithm_globals.random_seed)\n",
    "    \n",
    "    current_dataset = description\n",
    "    \n",
    "    # Initialize VQC for the adjusted feature map and ansatz\n",
    "    vqc = VQC(\n",
    "        feature_map=feature_map,\n",
    "        ansatz=ansatz,\n",
    "        optimizer=optimizer,\n",
    "        callback=callback_graph\n",
    "    )\n",
    "    \n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "    vqc.fit(train_features, train_labels)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Scores\n",
    "    train_score_quantum = vqc.score(train_features, train_labels)\n",
    "    test_score_quantum = vqc.score(test_features, test_labels)\n",
    "    vqc_scores[description] = {'Training Score': train_score_quantum,'Test Score': test_score_quantum}\n",
    "\n",
    "    # Store elapsed time for training\n",
    "    training_times[description] = elapsed_time\n",
    "    print(f\"Training time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd0aae-d48a-44fc-b796-270cbfbbdfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqc_scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c783f-8a62-4299-9313-89f489b670b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_feature_test_score_quantum = vqc_scores['Full Features']['Test Score']\n",
    "quantum_differences = {desc: full_feature_test_score_quantum - vqc_scores[desc]['Test Score'] \n",
    "                       for desc in vqc_scores if desc != \"Full Features\"}\n",
    "descriptions_quantum = list(quantum_differences.keys())\n",
    "quantum_diff_values = list(quantum_differences.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3a0f4a-1348-4999-bcb1-1ac70f0b8613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming vqc_scores is structured correctly and populated with your quantum models' scores\n",
    "full_feature_test_score_quantum = vqc_scores['Full Features']['Test Score']\n",
    "\n",
    "# Calculate the differences from the full feature set for quantum models\n",
    "quantum_differences = {desc: full_feature_test_score_quantum - vqc_scores[desc]['Test Score'] \n",
    "                       for desc in vqc_scores if desc != \"Full Features\"}\n",
    "\n",
    "# Prepare for plotting\n",
    "descriptions_quantum = list(quantum_differences.keys())\n",
    "quantum_diff_values = list(quantum_differences.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4048484d-8973-4716-a216-526012361a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(descriptions_quantum, quantum_diff_values, color='blue')\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Difference in Test Accuracy from Full Features')\n",
    "plt.title('Impact of Missing Features on Quantum Model Test Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "# Print scores and differences for quantum models\n",
    "print(\"Quantum Scores and Differences from Full Features:\")\n",
    "for description, scores in vqc_scores.items():\n",
    "        test_score_quantum = scores['Test Score']\n",
    "        difference_quantum = full_feature_test_score_quantum - test_score_quantum\n",
    "        print(f\"{description}:\")\n",
    "        print(f\"  - Quantum VQC on the training dataset: {scores['Training Score']:.2f}\")\n",
    "        print(f\"  - Quantum VQC on the test dataset:     {test_score_quantum:.2f}\")\n",
    "        print(f\"  - Difference from Full Features test score: {difference:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c49cee-360b-429d-ad46-bf7ba1c06aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame({\n",
    "    'Feature Set': classical_scores.keys(),\n",
    "    'Classical Training Score': [score['Training'] for score in classical_scores.values()],\n",
    "    'Classical Test Score': [score['Test'] for score in classical_scores.values()],\n",
    "    'Quantum Training Score': [score['Training Score'] for score in vqc_scores.values()],\n",
    "    'Quantum Test Score': [score['Test Score'] for score in vqc_scores.values()]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160ce0f5-177d-4a1e-9184-0f82c951981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame({\n",
    "    'Feature Set': classical_scores.keys(),\n",
    "    'Classical Training Score': [score['Training'] for score in classical_scores.values()],\n",
    "    'Classical Test Score': [score['Test'] for score in classical_scores.values()],\n",
    "    'Classical Difference': [classical_test_diff[desc] for desc in classical_scores.keys()],\n",
    "    'Quantum Training Score': [score['Training Score'] for score in vqc_scores.values()],\n",
    "    'Quantum Test Score': [score['Test Score'] for score in vqc_scores.values()],\n",
    "    'Quantum Difference': [quantum_test_diff[desc] for desc in vqc_scores.keys()]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2d1def-0837-4e86-88d8-a66fe00407f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d185edd-c21c-4ec3-9bdf-bb549eff96d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_groups = \n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "opacity = 0.8\n",
    "\n",
    "# Classical Model Bars\n",
    "classical_bars = plt.bar(index, classical_diff_values, bar_width, alpha=opacity, color='r', label='Classical')\n",
    "\n",
    "# Quantum Model Bars\n",
    "quantum_bars = plt.bar(index + bar_width, quantum_diff_values, bar_width, alpha=opacity, color='b', label='Quantum')\n",
    "\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Difference from Full Feature Set')\n",
    "plt.title('Test Score Differences: Quantum vs Classical Models')\n",
    "plt.xticks(index + bar_width / 2, descriptions, rotation=45)  # Center labels between bars\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to make room for the rotated x-axis labels\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9535a9fb-6f48-46d4-80f8-4a8c3b6a2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "total_runtime = end_time - start_time\n",
    "print(f\"Total Notebook Runtime: {total_runtime} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daec7227-94d9-474a-92bc-9d1f0c61b3be",
   "metadata": {},
   "source": [
    "Resources: \n",
    "\n",
    "- [IBM](https://qiskit.org/ecosystem/machine-learning/tutorials/02a_training_a_quantum_model_on_a_real_dataset.html) \n",
    "- [QML Summer School 2021](https://youtube.com/playlist?list=PLOFEBzvs-VvqJwybFxkTiDzhf5E11p8BI&si=zTRQ8YI7knonf7Nw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
